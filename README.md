# ğŸš” Vehicle Theft Data Engineering Project â€” Databricks Medallion Architecture

## ğŸš€ Overview
This beginner-friendly end-to-end project demonstrates how to design and implement an **end-to-end ETL pipeline** on **Azure Databricks**, following the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) to transform raw vehicle theft records into **business-ready analytical data**.

The solution automates **data ingestion, transformation, and enrichment** using **PySpark**, **Delta Lake**, and **Unity Catalog** to deliver high-quality, governed data for downstream analysis â€” enabling insights into theft patterns, hotspot areas, recovery timelines, and risk trends.

---

## ğŸ§© Architecture Overview
![Vehicle Theft Databricks Architecture](assets/Vehicle%20Theft%20Daigram.png)

The above architecture visualizes the **data flow and governance setup**:
- **Azure Data Factory (ADF)** handles data ingestion from multiple CSV sources into the **Landing Layer**.
- **Databricks with Unity Catalog** manages schema evolution and processing across **Bronze**, **Silver**, and **Gold** layers.
- **Power BI / Tableau** consumes the curated Gold data for visualization.

---

## âš™ï¸ Project Workflow

### 1ï¸âƒ£ Data Ingestion (Bronze Layer)
- Ingested raw data from CSV sources â€” `locations.csv`, `make_details.csv`, and `stolen_vehicles.csv` â€” using **ADF Copy Data** activity.  
- Each dataset was stored as **Parquet files** in ADLS Gen2 under the **Bronze container**.

### 2ï¸âƒ£ Data Transformation and Cleaning (Silver Layer)
- Cleaned and standardized vehicle data using **PySpark**:
  - Removed duplicates and nulls.  
  - Standardized string casing and timestamp formats.  
  - Joined location and make details with the main stolen vehicle dataset.
- Stored the transformed datasets in **Silver Delta tables** for validation and incremental loads.

### 3ï¸âƒ£ Business-Ready Data (Gold Layer)
- Created aggregated datasets to support **analytics and KPI dashboards**
- Generated **Gold Delta tables** for downstream BI tools.

---

## ğŸ§° Tech Stack

| Category | Tools & Technologies |
|-----------|----------------------|
| Data Lake | Azure Data Lake Storage (ADLS Gen2) |
| Orchestration | Azure Data Factory |
| Processing | Azure Databricks (PySpark, Delta Lake) |
| Governance | Unity Catalog |
| Language | Python (PySpark) |
| Visualization | Power BI / Tableau |

---

## ğŸ“’ Notebooks

| Layer | Notebook | Description |
|--------|-----------|-------------|
| ğŸŸ¤ Bronze | [01. Data Ingestion](notebooks/01.%20Data%20Ingestion.html) | Loads CSV data from ADF Landing Layer into Bronze Delta tables |
| âšª Silver | [02. Data Transformation and Cleaning](notebooks/02.%20Data%20Transformation%20and%20Cleaning.html) | Cleans, validates, and enriches vehicle theft datasets |
| ğŸŸ¡ Gold | [03. Business Ready Data](notebooks/03.%20Business%20Ready%20Data.html) | Aggregates and prepares data for BI dashboards |

---

## ğŸ’¾ Data Folder

ğŸ“ **`/data`**

| File Name | Description |
|------------|-------------|
| `locations.csv` | Contains geospatial and administrative location mappings |
| `make_details.csv` | Vehicle make and model information |
| `stolen_vehicles.csv` | Theft records including theft date, recovery date, and location |

All datasets are ingested by ADF into **ADLS Landing Layer**, then processed through **Databricks**.

---

## ğŸ“Š ADF Pipeline Visualization
![ADF Copy Activity](assets/Screenshot%202025-11-11%20195225.png)

The **ADF Copy Data activities** orchestrate ingestion from multiple sources to the Bronze container in ADLS, forming the foundation for the Databricks ETL process.

---

## ğŸ’¡ Key Learnings
- Built an **end-to-end Lakehouse ETL pipeline** using Databricks.  
- Implemented **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) with Delta Lake.  
- Enforced **data governance and lineage** through Unity Catalog.  
- Automated ingestion pipelines using **ADF**.  
- Derived insights on **vehicle theft hotspots and recovery analytics** using BI tools.

---

## ğŸ§­ Outcome
âœ… Automated data pipeline from raw CSVs to business-ready analytics  
âœ… Unified and governed data model for reliable insights  
âœ… BI-ready tables optimized for Power BI & Tableau  
âœ… Scalable architecture for future datasets and schema changes  

---

## ğŸªœ Folder Description

| Folder | Description |
|---------|-------------|
| `data/` | Raw input files (`locations.csv`, `make_details.csv`, `stolen_vehicles.csv`) |
| `notebooks/` | Databricks notebooks for each Medallion layer |
| `assets/` | Architecture and pipeline visuals |
| `README.md` | Complete project documentation |

---

## ğŸ‘¨â€ğŸ’» Author
**Rishikesh Gundla**  
_Senior Business Intelligence Engineer | Data Engineering & Analytics Enthusiast_  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/rishikeshgundla)
